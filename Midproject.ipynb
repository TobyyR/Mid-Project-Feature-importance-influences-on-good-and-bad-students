{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install category_encoders # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install shap # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install xgboost # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math as m\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import statsmodels.api as sm\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.inspection import permutation_importance\n",
    "import shap\n",
    "import category_encoders as ce\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Expanded_data_with_more_features.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop nans like talked about when chosing the dataset \n",
    "df = df.dropna()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"avg_total_score\"] = round((df[\"MathScore\"] + df[\"ReadingScore\"] + df[\"WritingScore\"])/ 3)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([\"Unnamed: 0\"], axis=1, inplace=True)\n",
    "\n",
    "mapping = {'< 5': 5, '5 - 10': 10, '> 10': 15}\n",
    "\n",
    "# Replace values in the specified column\n",
    "df[\"WklyStudyHours\"] = df[\"WklyStudyHours\"].replace(mapping)\n",
    "\n",
    "# Check the result\n",
    "print(df[\"WklyStudyHours\"].unique())\n",
    "df[\"WklyStudyHours\"].astype(\"int\")\n",
    "df[\"NrSiblings\"].astype(\"int\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the outliers\n",
    "def detect_outliers_quantile_low(data = df, lower_quantile=0.1, upper_quantile=1):\n",
    "    lower_bound = data.quantile(lower_quantile)\n",
    "    upper_bound = data.quantile(upper_quantile)\n",
    "    outliers = data[(data < lower_bound) | (data > upper_bound)]\n",
    "    return outliers\n",
    "\n",
    "outliers_total_score = detect_outliers_quantile_low(df[\"avg_total_score\"])\n",
    "# cleaned df\n",
    "cleaned_df_low = df[~df[\"avg_total_score\"].isin(outliers_total_score)] # logical operqtor ~ flips values in boolean series\n",
    "# outlier df\n",
    "outliers_df_low = df[df[\"avg_total_score\"].isin(outliers_total_score)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df_low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers_df_low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers_quantile_high(data = df, lower_quantile=0, upper_quantile=0.95):\n",
    "    lower_bound = data.quantile(lower_quantile)\n",
    "    upper_bound = data.quantile(upper_quantile)\n",
    "    outliers = data[(data < lower_bound) | (data > upper_bound)]\n",
    "    return outliers\n",
    "\n",
    "\n",
    "outliers_total_score = detect_outliers_quantile_high(df[\"avg_total_score\"])\n",
    "cleaned_df_high = df[~df[\"avg_total_score\"].isin(outliers_total_score)]\n",
    "outliers_df_high = df[df[\"avg_total_score\"].isin(outliers_total_score)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df_high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers_df_high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers_quantile(data = df, lower_quantile=0.05, upper_quantile=0.95):\n",
    "    lower_bound = data.quantile(lower_quantile)\n",
    "    upper_bound = data.quantile(upper_quantile)\n",
    "    outliers = data[(data < lower_bound) | (data > upper_bound)]\n",
    "    return outliers\n",
    "\n",
    "outliers_total_score = detect_outliers_quantile(df[\"avg_total_score\"])\n",
    "cleaned_df = df[~df[\"avg_total_score\"].isin(outliers_total_score)]\n",
    "outliers_df = df[df[\"avg_total_score\"].isin(outliers_total_score)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers_df_high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers_df_low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers_df_low[\"avg_total_score\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# safe as csv\n",
    "\n",
    "cleaned_df.to_csv(\"cleaned_df\", index= False)\n",
    "outliers_df_high.to_csv(\"outliers_df_high\", index= False)\n",
    "outliers_df_low.to_csv(\"outliers_df_low\", index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Separate features and target\n",
    "X = cleaned_df.drop(['avg_total_score', \"MathScore\", \"ReadingScore\", \"WritingScore\"], axis=1)  # Features\n",
    "y = cleaned_df['avg_total_score']  # Target\n",
    "\n",
    "# One-hot encode categorical columns using pandas get_dummies\n",
    "X_encoded = pd.get_dummies(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=.25, random_state=42)\n",
    "\n",
    "# Initialize Random Forest Classifier\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Fit the model\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = rf.feature_importances_\n",
    "\n",
    "# Create DataFrame to hold feature names and importances\n",
    "importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importances})\n",
    "\n",
    "# Sort the DataFrame by importance values\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Print the sorted feature importances\n",
    "print(importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_importances = pd.Series(rf.feature_importances_, index=X_train.columns)\n",
    "global_importances.sort_values(ascending=True, inplace=True)\n",
    "global_importances.plot.barh(color='green')\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"Global Feature Importance - Built-in Method\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cleaned_df.drop(['avg_total_score', \"MathScore\", \"ReadingScore\", \"WritingScore\"], axis=1)  # Features\n",
    "y = cleaned_df['avg_total_score']  # Target\n",
    "\n",
    "# One-hot encode categorical columns using pandas get_dummies\n",
    "X_encoded = pd.get_dummies(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=.25, random_state=42)\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "result = permutation_importance(rf, X_test, y_test, n_repeats=10, random_state=42)\n",
    "\n",
    "perm_importances = result.importances_mean\n",
    "perm_std = result.importances_std\n",
    "sorted_idx = perm_importances.argsort()\n",
    "feature_names = X_test.columns\n",
    "\n",
    "pd.DataFrame({'Importance': perm_importances, 'Std': perm_std}, index=feature_names[sorted_idx]).sort_values('Importance',ascending=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = outliers_df_low.drop(['avg_total_score', \"MathScore\", \"ReadingScore\", \"WritingScore\"], axis=1)  # Features\n",
    "y = outliers_df_low['avg_total_score']  # Target\n",
    "\n",
    "# One-hot encode categorical columns using pandas get_dummies\n",
    "X_encoded = pd.get_dummies(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=.25, random_state=42)\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(f\"model score on training data: {model.score(X_train, y_train)}\")\n",
    "print(f\"model score on testing data: {model.score(X_test, y_test)}\")\n",
    "\n",
    "importances = model.feature_importances_\n",
    "\n",
    "indices = np.argsort(importances)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.barh(range(len(importances)), importances[indices])\n",
    "ax.set_yticks(range(len(importances)))\n",
    "_ = ax.set_yticklabels(np.array(X_train.columns)[indices])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = outliers_df_high.drop(['avg_total_score', \"MathScore\", \"ReadingScore\", \"WritingScore\"], axis=1)  # Features\n",
    "y = outliers_df_high['avg_total_score']  # Target\n",
    "\n",
    "# One-hot encode categorical columns using pandas get_dummies\n",
    "X_encoded = pd.get_dummies(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=.25, random_state=42)\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(f\"model score on training data: {model.score(X_train, y_train)}\")\n",
    "print(f\"model score on testing data: {model.score(X_test, y_test)}\")\n",
    "\n",
    "importances = model.feature_importances_\n",
    "\n",
    "indices = np.argsort(importances)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.barh(range(len(importances)), importances[indices])\n",
    "ax.set_yticks(range(len(importances)))\n",
    "_ = ax.set_yticklabels(np.array(X_train.columns)[indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cleaned_df.drop(['avg_total_score', \"MathScore\", \"ReadingScore\", \"WritingScore\"], axis=1)  # Features\n",
    "y = cleaned_df['avg_total_score']  # Target\n",
    "\n",
    "# One-hot encode categorical columns using pandas get_dummies\n",
    "X_encoded = pd.get_dummies(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=.25, random_state=42)\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(f\"model score on training data: {model.score(X_train, y_train)}\")\n",
    "print(f\"model score on testing data: {model.score(X_test, y_test)}\")\n",
    "\n",
    "importances = model.feature_importances_\n",
    "\n",
    "indices = np.argsort(importances)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.barh(range(len(importances)), importances[indices])\n",
    "ax.set_yticks(range(len(importances)))\n",
    "_ = ax.set_yticklabels(np.array(X_train.columns)[indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned_no_score = cleaned_df.drop([\"ReadingScore\",\"WritingScore\", \"MathScore\"], axis=1)\n",
    "df_cleaned_no_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = df_cleaned_no_score[\"WklyStudyHours\"].unique()\n",
    "print(unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_cleaned_no_score.drop('avg_total_score', axis=1)  # Features\n",
    "y = df_cleaned_no_score['avg_total_score']  # Target\n",
    "\n",
    "# One-hot encode categorical columns using pandas get_dummies\n",
    "X_encoded = pd.get_dummies(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=.25, random_state=42)\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(f\"model score on training data: {model.score(X_train, y_train)}\")\n",
    "print(f\"model score on testing data: {model.score(X_test, y_test)}\")\n",
    "\n",
    "importances = model.feature_importances_\n",
    "\n",
    "indices = np.argsort(importances)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.barh(range(len(importances)), importances[indices])\n",
    "ax.set_yticks(range(len(importances)))\n",
    "_ = ax.set_yticklabels(np.array(X_train.columns)[indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers_ova = pd.concat([outliers_df_high,  outliers_df_low])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers_ova"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers_ova.drop([\"ReadingScore\",\"WritingScore\", \"MathScore\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_student(score):\n",
    "    if score < 50:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers_ova[\"classification\"] = outliers_ova[\"avg_total_score\"].apply(classify_student)\n",
    "outliers_ova"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"classification\"] = df[\"avg_total_score\"].apply(classify_student)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers_ova"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X = outliers_ova.drop('classification', axis=1)\n",
    "y = outliers_ova[\"classification\"]\n",
    "\n",
    "\n",
    "# Encoding categorical features and scaling numerical features\n",
    "numerical_features = ['NrSiblings','WklyStudyHours']\n",
    "categorical_features = ['Gender', 'EthnicGroup', 'ParentEduc', \"LunchType\", \"TestPrep\", \"ParentMaritalStatus\", \"PracticeSport\", \"IsFirstChild\", \"TransportMeans\"]\n",
    "\n",
    "# Creating transformers\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer()),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# Column transformer to apply the transformations\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "\n",
    "\n",
    "model = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "('model', LogisticRegression(class_weight=\"balanced\"))])\n",
    "    \n",
    "# Training the model \n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "    \n",
    "# Predicting \n",
    "y_pred_test = model.predict(X_test)\n",
    "y_pred_train = model.predict(X_train)\n",
    "reporttest = classification_report(y_test, y_pred_test)\n",
    "reporttrain = classification_report(y_train, y_pred_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"test\", reporttest)\n",
    "print(\"train\", reporttrain)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_logistic_regression(data, target_col, drop_col,  test_size=0.2, random_state=42):\n",
    "    data = data.drop(columns=[drop_col])\n",
    "    # Define features and target variable\n",
    "\n",
    "\n",
    "    X = data.drop(columns=[target_col])\n",
    "    y = (data[target_col])\n",
    "\n",
    "    # Encoding categorical variables if needed (e.g., using one-hot encoding)\n",
    "    X = pd.get_dummies(X)\n",
    "\n",
    "    # Splitting the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    # Initialize and fit the logistic regression model\n",
    "    model = LogisticRegression(class_weight=\"balanced\")\n",
    "    model.fit(X_train, y_train)\n",
    "    # Predicting \n",
    "    y_pred_test = model.predict(X_test)\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    reporttest = classification_report(y_test, y_pred_test)\n",
    "    reporttrain = classification_report(y_train, y_pred_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(\"test\", reporttest)\n",
    "    print(\"train\", reporttrain)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_logistic_regression(outliers_ova, target_col=\"classification\", drop_col=\"avg_total_score\", test_size= 0.2, random_state= 42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def confidence(confidence_level):\n",
    "    n = len(outliers_ova)\n",
    "    p = outliers_ova[\"classification\"].sum()/n\n",
    "    t = stats.t.ppf(confidence_level + (1- confidence_level)/2, n-1)\n",
    "    error = t * m.sqrt(p * (1-p)/n)\n",
    "    CI = [p - error, p + error]\n",
    "    return CI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence(0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X = df_cleaned_no_score.drop(['avg_total_score'], axis=1)  # Features\n",
    "y = df_cleaned_no_score['avg_total_score']  # Target\n",
    "\n",
    "X = pd.get_dummies(X)\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "# split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# fit model on all training data\n",
    "\n",
    "\n",
    "\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "plot_importance(model)\n",
    "\n",
    "\n",
    "print(f\"model score on training data: {model.score(X_train, y_train)}\")\n",
    "print(f\"model score on testing data: {model.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = outliers_ova.drop(['avg_total_score'], axis=1)\n",
    "y = outliers_ova['avg_total_score']\n",
    "\n",
    "X = pd.get_dummies(X)\n",
    "\n",
    "# split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# fit model on all training data\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "# make predictions for test data and evaluate\n",
    "plot_importance(model)\n",
    "\n",
    "\n",
    "print(f\"model score on training data: {model.score(X_train, y_train)}\")\n",
    "print(f\"model score on testing data: {model.score(X_test, y_test)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = outliers_df_high.drop(['avg_total_score', \"MathScore\", \"ReadingScore\", \"WritingScore\", \"NrSiblings\"], axis=1) # drop NrSiblings because I can not explain if more or less siblings are influential yet\n",
    "y = outliers_df_high['avg_total_score']\n",
    "\n",
    "X = pd.get_dummies(X)\n",
    "\n",
    "# split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# fit model on all training data\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "# make predictions for test data and evaluate\n",
    "plot_importance(model)\n",
    "\n",
    "\n",
    "print(f\"model score on training data: {model.score(X_train, y_train)}\")\n",
    "print(f\"model score on testing data: {model.score(X_test, y_test)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = outliers_df_low.drop(['avg_total_score', \"MathScore\", \"ReadingScore\", \"WritingScore\", \"NrSiblings\"], axis=1) # drop NrSiblings because I can not explain if more or less siblings are influential yet\n",
    "y = outliers_df_low['avg_total_score']\n",
    "\n",
    "X = pd.get_dummies(X)\n",
    "\n",
    "# split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# fit model on all training data\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "# make predictions for test data and evaluate\n",
    "plot_importance(model)\n",
    "\n",
    "\n",
    "print(f\"model score on training data: {model.score(X_train, y_train)}\")\n",
    "print(f\"model score on testing data: {model.score(X_test, y_test)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
